{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sfip2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sfip2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "import torchtext.vocab as vocab\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data from CSV file\n",
    "# data = pd.read_csv(\"input/twitter_sentiment_data.csv\")\n",
    "\n",
    "# # Define features\n",
    "# message_text = data[\"message\"]\n",
    "# sentiment_value = (data[\"sentiment\"]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenization\n",
    "# tokenizer = get_tokenizer(\"basic_english\")  # Basic tokenizer for English text\n",
    "# tokenized_data = [tokenizer(text.lower()) for text in message_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text_sklearn(text):\n",
    "\n",
    "#     # Define a regular expression pattern to match \"RT\" at the beginning of the string followed by any characters up to \":\"\n",
    "#     pattern = r'^RT.*?:'\n",
    "\n",
    "#     # Use re.sub() to replace the matched pattern with an empty string\n",
    "#     text = re.sub(pattern, '', text)\n",
    "    \n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "#     # Remove punctuation and special characters\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before data processing and stemming: ['@tiniebeany', 'climate', 'change', 'is', 'an', 'interesting', 'hustle', 'as', 'it', 'was', 'global', 'warming', 'but', 'the', 'planet', 'stopped', 'warming', 'for', '15', 'yes', 'while', 'the', 'suv', 'boom']\n",
      "After data processing and stemming: ['tiniebeany', 'climate', 'change', 'interesting', 'hustle', 'global', 'warming', 'planet', 'stopped', 'warming', 'yes', 'suv', 'boom']\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Remove stopwords from tokenized data\n",
    "# filtered_data = []\n",
    "\n",
    "# # Print example before\n",
    "# print(\"Before data processing and stemming:\", tokenized_data[0])\n",
    "\n",
    "# for sentence in tokenized_data:\n",
    "#     # Remove stopwords & empty string from the sentence\n",
    "#     filtered_sentence = [preprocess_text_sklearn(word) for word in sentence if word.lower() not in stop_words]\n",
    "#     filtered_sentence = [word for word in filtered_sentence if word.strip() != \"\"]\n",
    "#     filtered_data.append(filtered_sentence)\n",
    "\n",
    "# # Print example after \n",
    "# print(\"After data processing and stemming:\", filtered_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_name = 'glove'  # or 'word2vec'\n",
    "embedding_dim = 50\n",
    "num_filters = 50\n",
    "filter_sizes = [2, 3, 4]\n",
    "output_dim = 1\n",
    "dropout = 0.3\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:13<00:00, 28916.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example padded data: tensor([[ 0.2085,  0.7098, -0.4768,  ...,  0.9483,  0.2803,  0.6592],\n",
      "        [ 0.0425,  0.1255, -0.0661,  ...,  0.5478,  0.3289,  0.6525],\n",
      "        [ 0.6185,  0.6425, -0.4655,  ..., -0.2756,  0.3090,  0.4850],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# # Load pre-trained word embeddings\n",
    "# glove = vocab.GloVe(name='6B', dim=embedding_dim)\n",
    "\n",
    "# # Convert tokenized data to indices using pre-trained embeddings\n",
    "# indexed_data = [[glove[word].tolist() for word in sentence if word in glove.stoi] for sentence in tokenized_data]\n",
    "\n",
    "# # Convert indexed data to PyTorch tensors\n",
    "# data_tensors = [torch.tensor(sentence, dtype=torch.float32) for sentence in indexed_data]\n",
    "\n",
    "# # Pad sequences to ensure uniform length (optional, depending on your dataset)\n",
    "# padded_data = nn.utils.rnn.pad_sequence(data_tensors, batch_first=True)\n",
    "\n",
    "# # Print Example\n",
    "# print(\"Example padded data:\", padded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Word Embedding\n",
    "word_embedding = vocab.GloVe(name='6B', dim=embedding_dim)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\") \n",
    "\n",
    "# Data\n",
    "data = pd.read_csv(\"input/twitter_sentiment_data.csv\")\n",
    "message_texts = data[\"message\"]\n",
    "#sentiment_values = (data[\"sentiment\"]>0.5).astype(int)\n",
    "sentiment_values = data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message Text Shape: (43943,)\n",
      "Sentiment Value Shape: (43943,)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tweet_texts, sentiments, tokenizer, stopwords, word_embedding, max_length):\n",
    "        self.tweet_texts = tweet_texts\n",
    "        self.sentiments = sentiments\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stopwords = stopwords\n",
    "        self.word_embedding = word_embedding\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get tweet text and sentiment value for the given index\n",
    "        tweet_text = self.tweet_texts[idx]\n",
    "        sentiment = self.sentiments[idx]\n",
    "        \n",
    "        # Perform tokenization\n",
    "        tokens = self.tokenizer(tweet_text)\n",
    "        \n",
    "        # Perform stop word removal\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "\n",
    "        # Pad the tokenized sentence with empty strings\n",
    "        padded_tokenized_sentence = tokens + [''] * (max_length - len(tokens))\n",
    "        \n",
    "        # Perform word embedding\n",
    "        embeddings = [self.word_embedding[token] for token in padded_tokenized_sentence]\n",
    "        \n",
    "        # Convert sentiment to tensor\n",
    "        sentiment_tensor = torch.tensor([sentiment], dtype=torch.float32)\n",
    "\n",
    "        padded_embeddings = pad_sequence(embeddings, batch_first=True, padding_value=0.0)\n",
    "        \n",
    "        # Convert embeddings to tensor\n",
    "        embeddings_tensor = torch.tensor(padded_embeddings)\n",
    "        \n",
    "        return embeddings_tensor, sentiment_tensor\n",
    "\n",
    "print(\"Message Text Shape:\", message_texts.shape)\n",
    "print(\"Sentiment Value Shape:\",sentiment_values.shape)\n",
    "\n",
    "# Find the maximum length of strings in the Series\n",
    "max_length = message_texts.str.len().max()\n",
    "\n",
    "# Create a dataset \n",
    "dataset = MyDataset(message_texts, sentiment_values, tokenizer, stop_words, word_embedding, max_length)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "Inputs dimensions: torch.Size([623, 50])\n",
      "Labels dimensions: torch.Size([1])\n",
      "\n",
      "Sample 2\n",
      "Inputs dimensions: torch.Size([623, 50])\n",
      "Labels dimensions: torch.Size([1])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sfip2\\AppData\\Local\\Temp\\ipykernel_16336\\3645350463.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_tensor = torch.tensor(padded_embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Checking that samples have the same shape before feeding into the model\n",
    "for i in range(2):  \n",
    "    inputs, labels = dataset[i]\n",
    "    print(\"Sample\", i+1)\n",
    "    print(\"Inputs dimensions:\", inputs.size())\n",
    "    print(\"Labels dimensions:\", labels.size())\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        conv_outputs = [torch.relu(conv(x.transpose(1, 2))) for conv in self.conv_layers]\n",
    "        \n",
    "        # Max pooling\n",
    "        pooled_outputs = [torch.max(conv, 2)[0] for conv in conv_outputs]\n",
    "        \n",
    "        # Concatenate pooled outputs\n",
    "        cat = torch.cat(pooled_outputs, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        cat = self.dropout(cat)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        output = self.fc(cat)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CNN(embedding_dim, num_filters, filter_sizes, output_dim, dropout)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Convert padded data to tensor\n",
    "# input_data = torch.tensor(padded_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sfip2\\AppData\\Local\\Temp\\ipykernel_16336\\3645350463.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embeddings_tensor = torch.tensor(padded_embeddings)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate over the training dataset in batches\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Zero the gradients\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass: compute predicted outputs by passing inputs to the model\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m padded_tokenized_sentence \u001b[38;5;241m=\u001b[39m tokens \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Perform word embedding\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpadded_tokenized_sentence\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert sentiment to tensor\u001b[39;00m\n\u001b[0;32m     34\u001b[0m sentiment_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([sentiment], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m padded_tokenized_sentence \u001b[38;5;241m=\u001b[39m tokens \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m (max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Perform word embedding\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m padded_tokenized_sentence]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert sentiment to tensor\u001b[39;00m\n\u001b[0;32m     34\u001b[0m sentiment_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([sentiment], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\sfip2\\OneDrive\\Documents\\Christina\\UW Machine Learning\\ML-ClimateStatementSorting\\UW-ML\\Lib\\site-packages\\torchtext\\vocab\\vectors.py:65\u001b[0m, in \u001b[0;36mVectors.__getitem__\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi[token]]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_init(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the loss for this epoch\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Iterate over the training dataset in batches\n",
    "    for inputs, labels in data_loader:  \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters (weights) of the model\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for this batch\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss for this epoch\n",
    "    epoch_loss /= len(data_loader)\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UW-ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
